{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from dataset_operations import InputDataset, custom_collate_fn, get_transform, trainModel, read_mean_std\n",
    "\n",
    "tile_size = 512\n",
    "\n",
    "parent_dir = \"rois2/\"\n",
    "img_dir = parent_dir + \"images/\"\n",
    "tile_dir = parent_dir + \"tiles/\"\n",
    "model_dir = parent_dir + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  [0.77296131, 0.71338682, 0.68258603]\n",
      "std:  [0.16751114, 0.19680141, 0.21965831]\n",
      "11551\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "mean, std = read_mean_std(parent_dir + 'images/')\n",
    "\n",
    "val_dataloader = pickle.load(open(model_dir + 'val_dataloader.pkl', 'rb'))\n",
    "full_dataloader = pickle.load(open(model_dir + \"full_dataloader.pkl\", \"rb\"))\n",
    "\n",
    "print(len(full_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(num_classes=2)\n",
    "model = model.to('cuda')\n",
    "model.load_state_dict(torch.load('rois2/models/round1_model_epoch77.pth'))\n",
    "epoch_begin = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model of epoch 8 loaded\n",
      "Training started\n",
      "     Epoch[9/50], train batch[100/11551], bbox loss: 0.547163313627243, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[200/11551], bbox loss: 0.43985614661247496, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[300/11551], bbox loss: 0.20190012454986572, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[400/11551], bbox loss: 0.03745986521244049, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[500/11551], bbox loss: 0.5415526628494263, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[600/11551], bbox loss: 1.0055722925398085, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[700/11551], bbox loss: 0.7883853346786716, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[800/11551], bbox loss: 0.7302510142326355, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[900/11551], bbox loss: 2.6489717801411947, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1000/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1100/11551], bbox loss: 0.8409428596496582, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1200/11551], bbox loss: 0.7131896018981934, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1300/11551], bbox loss: 0.8391114473342896, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1400/11551], bbox loss: 0.7316668182611465, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1500/11551], bbox loss: 1.7928970504451442, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1600/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1700/11551], bbox loss: 0.6463674068450929, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1800/11551], bbox loss: 0.29720616340637207, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[1900/11551], bbox loss: 0.3326390752425561, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2000/11551], bbox loss: 1.5699278116226196, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2100/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2200/11551], bbox loss: 0.34995468954245246, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2300/11551], bbox loss: 0.5987716913223267, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2400/11551], bbox loss: 0.45961305871605873, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2500/11551], bbox loss: 0.8109605105026908, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2600/11551], bbox loss: 0.8345456350417364, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2700/11551], bbox loss: 0.975984063161456, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2800/11551], bbox loss: 0.9642601714414709, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[2900/11551], bbox loss: 0.38639476720024557, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3000/11551], bbox loss: 0.3801886002932276, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3100/11551], bbox loss: 0.4787560871669224, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3200/11551], bbox loss: 1.7336696982383728, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3300/11551], bbox loss: 0.5110686729694235, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3400/11551], bbox loss: 2.952629327774048, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3500/11551], bbox loss: 0.5486808311608102, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3600/11551], bbox loss: 0.33618511557579045, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3700/11551], bbox loss: 0.508243332306544, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3800/11551], bbox loss: 0.49458504759747046, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[3900/11551], bbox loss: 0.2633816816590049, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4000/11551], bbox loss: 1.0649373054504394, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4100/11551], bbox loss: 0.7656967282295227, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4200/11551], bbox loss: 0.292121016062223, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4300/11551], bbox loss: 0.7706459561983744, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4400/11551], bbox loss: 1.4334160731388974, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4500/11551], bbox loss: 2.3174511194229126, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4600/11551], bbox loss: 0.7071141621896199, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4700/11551], bbox loss: 0.8507379416157217, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4800/11551], bbox loss: 1.4296641910777372, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[4900/11551], bbox loss: 0.6390692833811045, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5000/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5100/11551], bbox loss: 0.6589329469771612, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5200/11551], bbox loss: 0.6070860491858588, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5300/11551], bbox loss: 0.8591758479242739, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5400/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5500/11551], bbox loss: 0.45572246753034135, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5600/11551], bbox loss: 0.5831083334409274, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5700/11551], bbox loss: 1.174401869275132, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5800/11551], bbox loss: 0.6848316269536172, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[5900/11551], bbox loss: 1.0800802914992622, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6000/11551], bbox loss: 0.31756565787575464, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6100/11551], bbox loss: 0.41611003379027045, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6200/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6300/11551], bbox loss: 0.3033774920872279, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6400/11551], bbox loss: 0.7735818347026562, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6500/11551], bbox loss: 1.2622558176517487, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6600/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6700/11551], bbox loss: 0.1810571402311325, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6800/11551], bbox loss: 0.453743314743042, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[6900/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7000/11551], bbox loss: 1.4580721974372863, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7100/11551], bbox loss: 0.8602415784483863, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7200/11551], bbox loss: 0.4004010558128357, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7300/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7400/11551], bbox loss: 1.8779901266098022, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7500/11551], bbox loss: 0.31203006551815915, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7600/11551], bbox loss: 0.26622092723846436, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7700/11551], bbox loss: 0.6508077383041382, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7800/11551], bbox loss: 0.8351036608219147, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[7900/11551], bbox loss: 1.151527639478445, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8000/11551], bbox loss: 0.5626492977142334, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8100/11551], bbox loss: 0.5188140471776326, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8200/11551], bbox loss: 1.622042465209961, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8300/11551], bbox loss: 1.139805439710617, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8400/11551], bbox loss: 0.5529793010038488, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8500/11551], bbox loss: 1.617980036470625, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8600/11551], bbox loss: 1.9033852815628052, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8700/11551], bbox loss: 0.4445688333362341, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8800/11551], bbox loss: 0.34323777964240626, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[8900/11551], bbox loss: 0.6131133483006405, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9000/11551], bbox loss: 0.30567115545272827, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9100/11551], bbox loss: 0.9714443930264177, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9200/11551], bbox loss: 0.7860377907752991, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9300/11551], bbox loss: 0.2640169331660638, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9400/11551], bbox loss: 0.7934609651565552, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9500/11551], bbox loss: 1.1601887564910085, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9600/11551], bbox loss: 0.904447207848231, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9700/11551], bbox loss: 1.6517605552306542, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9800/11551], bbox loss: 0.9052294832654297, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[9900/11551], bbox loss: 0.5514694452285767, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10000/11551], bbox loss: 0.5217193989526657, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10100/11551], bbox loss: 0.635542917251587, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10200/11551], bbox loss: 1.3029616355895997, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10300/11551], bbox loss: 1.4097700746435868, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10400/11551], bbox loss: 2.1036288982087914, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10500/11551], bbox loss: 0.6054634475708008, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10600/11551], bbox loss: 0.6115010253020695, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10700/11551], bbox loss: 2.5858361380440846, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10800/11551], bbox loss: 1.6858708652853966, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[10900/11551], bbox loss: 1.6315642529063754, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11000/11551], bbox loss: 1.0220262390725752, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11100/11551], bbox loss: 1.6160715818405151, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11200/11551], bbox loss: 0.38457120954990387, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11300/11551], bbox loss: 0.46730645927223, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11400/11551], bbox loss: 0.6555549621582031, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11500/11551], bbox loss: 0.2697143128940037, current learning rate:  0.001\n",
      "     Epoch[9/50], train batch[11551/11551], bbox loss: 0.0, current learning rate:  0.001\n",
      "Validation started\n",
      "     Epoch[9/50],  val batch[50/192], current recall = 732/786 = 0.9312977099236641 current precision = 732/171332 = 0.004272406789157892\n",
      "     Epoch[9/50],  val batch[100/192], current recall = 1487/1607 = 0.925326695706285 current precision = 1487/340946 = 0.004361394473025054\n",
      "     Epoch[9/50],  val batch[150/192], current recall = 2215/2390 = 0.9267782426778243 current precision = 2215/513022 = 0.004317553633177524\n",
      "     Epoch[9/50],  val batch[192/192], current recall = 2855/3070 = 0.9299674267100977 current precision = 2855/655593 = 0.004354836003435058\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "     Epoch[10/50], train batch[100/11551], bbox loss: 1.1342189631291797, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[200/11551], bbox loss: 0.6623892188072205, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[300/11551], bbox loss: 1.3653625529259443, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[400/11551], bbox loss: 0.0, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[500/11551], bbox loss: 0.35497643881373936, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[600/11551], bbox loss: 0.48950231075286865, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[700/11551], bbox loss: 2.244673013687134, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[800/11551], bbox loss: 1.1655643648571437, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[900/11551], bbox loss: 0.47217415010227876, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1000/11551], bbox loss: 0.4093731641769409, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1100/11551], bbox loss: 1.4841979980468751, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1200/11551], bbox loss: 0.8555278182029724, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1300/11551], bbox loss: 0.38060010969638824, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1400/11551], bbox loss: 0.11564169398375919, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1500/11551], bbox loss: 1.1941407918930054, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1600/11551], bbox loss: 0.48898164431254065, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1700/11551], bbox loss: 0.7269061207771301, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1800/11551], bbox loss: 0.4846894412205137, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[1900/11551], bbox loss: 0.5682481373660266, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2000/11551], bbox loss: 0.40943333857199726, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2100/11551], bbox loss: 1.5499450462929745, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2200/11551], bbox loss: 0.3019901911417643, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2300/11551], bbox loss: 0.8678030897589291, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2400/11551], bbox loss: 2.304068088531494, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2500/11551], bbox loss: 0.3494697277035032, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2600/11551], bbox loss: 0.43293383717536926, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2700/11551], bbox loss: 1.0611373876270493, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2800/11551], bbox loss: 0.23562653958797455, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[2900/11551], bbox loss: 1.556196572149501, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3000/11551], bbox loss: 0.5589234987894693, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3100/11551], bbox loss: 0.9296415820717812, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3200/11551], bbox loss: 1.4959694532056649, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3300/11551], bbox loss: 0.38744514403135877, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3400/11551], bbox loss: 0.3653205853921396, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3500/11551], bbox loss: 0.9240514971315861, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3600/11551], bbox loss: 0.6658312320709229, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3700/11551], bbox loss: 0.5386112788144279, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3800/11551], bbox loss: 0.8156186103820802, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[3900/11551], bbox loss: 1.00882827937603, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4000/11551], bbox loss: 0.6342926621437073, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4100/11551], bbox loss: 0.2547975480556488, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4200/11551], bbox loss: 0.7581917094556908, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4300/11551], bbox loss: 0.46623189856366415, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4400/11551], bbox loss: 0.30876828942980084, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4500/11551], bbox loss: 1.4832798540592194, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4600/11551], bbox loss: 0.5123410735811506, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4700/11551], bbox loss: 0.6936463788151741, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4800/11551], bbox loss: 0.6477418108419939, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[4900/11551], bbox loss: 0.836816018819809, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5000/11551], bbox loss: 0.5047568023204804, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5100/11551], bbox loss: 0.7892532488879035, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5200/11551], bbox loss: 1.121758859408529, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5300/11551], bbox loss: 0.6868932684883475, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5400/11551], bbox loss: 1.725106987006524, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5500/11551], bbox loss: 0.4424103260040283, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5600/11551], bbox loss: 0.4054383337497711, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5700/11551], bbox loss: 0.9217038090739931, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5800/11551], bbox loss: 0.3327268064022064, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[5900/11551], bbox loss: 0.6065117716789246, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6000/11551], bbox loss: 0.5371935403347016, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6100/11551], bbox loss: 0.551387139729091, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6200/11551], bbox loss: 1.03721567740043, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6300/11551], bbox loss: 0.6550973206758499, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6400/11551], bbox loss: 1.3553990840911867, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6500/11551], bbox loss: 0.4570802499850591, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6600/11551], bbox loss: 0.7507702042074764, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6700/11551], bbox loss: 0.6348516941070557, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6800/11551], bbox loss: 0.5593565639696623, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[6900/11551], bbox loss: 0.8921378175417582, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7000/11551], bbox loss: 0.328958997502923, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7100/11551], bbox loss: 0.24698926508426666, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7200/11551], bbox loss: 0.8952525799924678, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7300/11551], bbox loss: 2.651486237843831, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7400/11551], bbox loss: 5.362624168395996, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7500/11551], bbox loss: 0.12315354347229004, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7600/11551], bbox loss: 0.8132845560709635, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7700/11551], bbox loss: 0.9020867432866777, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7800/11551], bbox loss: 0.6705986633896828, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[7900/11551], bbox loss: 0.7029298742612202, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8000/11551], bbox loss: 0.0, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8100/11551], bbox loss: 0.6341808683731976, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8200/11551], bbox loss: 0.0, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8300/11551], bbox loss: 0.0, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8400/11551], bbox loss: 0.6184492707252502, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8500/11551], bbox loss: 0.8198904991149902, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8600/11551], bbox loss: 0.18580896854400636, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8700/11551], bbox loss: 1.2377351966558718, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8800/11551], bbox loss: 0.47819348743983675, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[8900/11551], bbox loss: 1.842092196146647, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9000/11551], bbox loss: 0.2848282814025879, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9100/11551], bbox loss: 0.8941354325839451, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9200/11551], bbox loss: 0.05531200394034386, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9300/11551], bbox loss: 0.6777311960856119, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9400/11551], bbox loss: 0.0, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9500/11551], bbox loss: 0.2944051921367645, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9600/11551], bbox loss: 2.0073216259479523, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9700/11551], bbox loss: 0.7073996054365279, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9800/11551], bbox loss: 0.6425914430083373, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[9900/11551], bbox loss: 0.8250971294584728, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10000/11551], bbox loss: 0.549577608704567, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10100/11551], bbox loss: 1.304229354361693, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10200/11551], bbox loss: 0.9062290092309315, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10300/11551], bbox loss: 0.43214782200208524, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10400/11551], bbox loss: 0.6950141191482544, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10500/11551], bbox loss: 1.3042553901672365, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10600/11551], bbox loss: 0.8332339003682137, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10700/11551], bbox loss: 0.6641709182573401, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10800/11551], bbox loss: 0.5933004637127337, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[10900/11551], bbox loss: 1.1192802515896885, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11000/11551], bbox loss: 1.302542755646365, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11100/11551], bbox loss: 0.6417393982410431, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11200/11551], bbox loss: 0.6880825757980347, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11300/11551], bbox loss: 0.272365465760231, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11400/11551], bbox loss: 0.5163186447960989, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11500/11551], bbox loss: 0.0, current learning rate:  0.0009990143508499217\n",
      "     Epoch[10/50], train batch[11551/11551], bbox loss: 1.737936019897461, current learning rate:  0.0009990143508499217\n",
      "Validation started\n",
      "     Epoch[10/50],  val batch[50/192], current recall = 725/806 = 0.8995037220843672 current precision = 725/199801 = 0.003628610467415078\n",
      "     Epoch[10/50],  val batch[100/192], current recall = 1435/1592 = 0.9013819095477387 current precision = 1435/401220 = 0.0035765913962414637\n",
      "     Epoch[10/50],  val batch[150/192], current recall = 2169/2398 = 0.9045037531276063 current precision = 2169/602018 = 0.003602882305844676\n",
      "     Epoch[10/50],  val batch[192/192], current recall = 2787/3070 = 0.9078175895765472 current precision = 2787/769590 = 0.0036214088020894244\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "     Epoch[11/50], train batch[100/11551], bbox loss: 0.31344767411549884, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[200/11551], bbox loss: 0.4204866886138916, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[300/11551], bbox loss: 0.9154762029647827, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[400/11551], bbox loss: 0.0, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[500/11551], bbox loss: 0.5720889451711074, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[600/11551], bbox loss: 0.742848590016365, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[700/11551], bbox loss: 0.4636981752183702, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[800/11551], bbox loss: 0.09701929241418839, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[900/11551], bbox loss: 0.5873817893174978, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1000/11551], bbox loss: 2.6438753455877304, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1100/11551], bbox loss: 0.66168452943525, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1200/11551], bbox loss: 1.7658201456069946, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1300/11551], bbox loss: 1.1562577486038208, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1400/11551], bbox loss: 0.0, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1500/11551], bbox loss: 0.8289642333984375, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1600/11551], bbox loss: 2.578021250665188, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1700/11551], bbox loss: 0.0, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1800/11551], bbox loss: 0.7872836879071067, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[1900/11551], bbox loss: 0.7317466302351519, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2000/11551], bbox loss: 0.8906386947631836, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2100/11551], bbox loss: 6.0677973971917085, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2200/11551], bbox loss: 0.4628620545069376, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2300/11551], bbox loss: 1.7513859272003174, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2400/11551], bbox loss: 1.2278093641454524, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2500/11551], bbox loss: 1.9733216762542725, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2600/11551], bbox loss: 0.4725508823476989, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2700/11551], bbox loss: 0.8390412880824163, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2800/11551], bbox loss: 0.3991539627313614, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[2900/11551], bbox loss: 0.264968603849411, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3000/11551], bbox loss: 0.9180800815423329, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3100/11551], bbox loss: 0.45356209327777225, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3200/11551], bbox loss: 0.26574332018693286, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3300/11551], bbox loss: 0.28181033333142597, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3400/11551], bbox loss: 0.3448641300201416, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3500/11551], bbox loss: 0.0, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3600/11551], bbox loss: 0.6228043138980865, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3700/11551], bbox loss: 1.3614536166191102, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3800/11551], bbox loss: 2.6972086429595947, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[3900/11551], bbox loss: 1.6002740361473777, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4000/11551], bbox loss: 1.204632823283856, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4100/11551], bbox loss: 0.0, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4200/11551], bbox loss: 0.7409942320414952, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4300/11551], bbox loss: 0.47495026964890324, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4400/11551], bbox loss: 0.44250616803765297, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4500/11551], bbox loss: 0.0, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4600/11551], bbox loss: 0.8504981729719373, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4700/11551], bbox loss: 0.6497292779386044, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4800/11551], bbox loss: 0.6843629827102025, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[4900/11551], bbox loss: 0.5879759887854258, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5000/11551], bbox loss: 2.5241563270489373, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5100/11551], bbox loss: 0.7835256099700928, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5200/11551], bbox loss: 0.2898868918418884, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5300/11551], bbox loss: 0.777746868133545, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5400/11551], bbox loss: 0.4379264391385592, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5500/11551], bbox loss: 2.1839718881406283, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5600/11551], bbox loss: 1.1324195861816406, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5700/11551], bbox loss: 0.7908996343612671, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5800/11551], bbox loss: 1.6236660555005074, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[5900/11551], bbox loss: 2.373831033706665, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6000/11551], bbox loss: 0.8857699164322443, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6100/11551], bbox loss: 0.8283119329384394, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6200/11551], bbox loss: 0.17429053783416748, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6300/11551], bbox loss: 0.23408966064453127, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6400/11551], bbox loss: 0.8076883686913384, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6500/11551], bbox loss: 4.706239700317383, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6600/11551], bbox loss: 0.07973015308380127, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6700/11551], bbox loss: 0.8280713707208633, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6800/11551], bbox loss: 0.9020170623605902, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[6900/11551], bbox loss: 0.3011869192123413, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7000/11551], bbox loss: 0.8022696905665927, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7100/11551], bbox loss: 0.28735742767651873, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7200/11551], bbox loss: 0.3539731527368227, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7300/11551], bbox loss: 0.3392981837193171, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7400/11551], bbox loss: 0.7106939378906698, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7500/11551], bbox loss: 1.3234812980744897, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7600/11551], bbox loss: 3.714411973953247, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7700/11551], bbox loss: 0.08459721505641937, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7800/11551], bbox loss: 2.1084528416395187, current learning rate:  0.0009960612933065818\n",
      "     Epoch[11/50], train batch[7900/11551], bbox loss: 1.7909259796142578, current learning rate:  0.0009960612933065818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43mparent_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43mepochs_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[43mepoch_begin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_begin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43mnms_thr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m           \u001b[49m\u001b[43miou_thr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m           \u001b[49m\u001b[43mchkpt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mround2_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m           \u001b[49m\u001b[43mloss_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_T_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AC\\UCLA\\UCLA HCI\\summer intern\\project code\\dataset_operations.py:680\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(model, parent_dir, train_dataloader, val_dataloader, epochs_num, epoch_begin, nms_thr, iou_thr, device, chkpt_name, loss_penalty, scheduler_T_max, lr_min, lr_init, lr_decay)\u001b[0m\n\u001b[0;32m    678\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m# Sets the gradients of all optimized tensors to zero\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    683\u001b[0m current_lr \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\2024summer\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\2024summer\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\2024summer\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\2024summer\\lib\\site-packages\\torch\\optim\\adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    178\u001b[0m         group,\n\u001b[0;32m    179\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         state_steps,\n\u001b[0;32m    186\u001b[0m     )\n\u001b[1;32m--> 188\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\anaconda\\envs\\2024summer\\lib\\site-packages\\torch\\optim\\adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\2024summer\\lib\\site-packages\\torch\\optim\\adamw.py:609\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    607\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    612\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainModel(model = model, \n",
    "           parent_dir=parent_dir, \n",
    "           train_dataloader=full_dataloader, \n",
    "           val_dataloader=val_dataloader, \n",
    "           epochs_num=50,\n",
    "           epoch_begin=epoch_begin, \n",
    "           nms_thr=0.5, \n",
    "           iou_thr=0.5, \n",
    "           device='cuda',\n",
    "           chkpt_name='round2_model',\n",
    "           loss_penalty=2,\n",
    "           lr_init=1e-3,\n",
    "            lr_decay=0.1,\n",
    "            lr_min=1e-6,\n",
    "            scheduler_T_max=50\n",
    "           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
